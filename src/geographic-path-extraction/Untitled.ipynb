{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "basepath = os.path.dirname(os.path.abspath('__file__'))\n",
    "datapath1 = os.path.dirname(basepath)\n",
    "datapath = os.path.dirname(datapath1)\n",
    "\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datapath = Path(__file__).resolve().parents[2]\n",
    "\n",
    "part = \"part1\"\n",
    "readfile = datapath + '/results/hugh-murray/part1/processed/part1-annotated-special.csv'\n",
    "data1 = pd.read_csv(readfile, sep='\\t', encoding='latin1', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "isInSection         float64\n",
      "isTravelVerb        float64\n",
      "isInTravelPhrase    float64\n",
      "sentence_final       object\n",
      "dtype: object\n",
      "Okay Done\n"
     ]
    }
   ],
   "source": [
    "data = data1[data1.Location.notnull()].reset_index()\n",
    "\n",
    "print(\"test\")\n",
    "#data['sentence1']= [word_tokenize(entry) for entry in data['sentence']]\n",
    "\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,row in data.iterrows():\n",
    "    \n",
    "    if row['Location'] in row['sectionTitle']:\n",
    "        is_in_section = 1\n",
    "    else:\n",
    "        is_in_section = 0\n",
    "        \n",
    "    #print(type(row['Travel verbs']))\n",
    "    if not isinstance(row['Travel verbs'],float):\n",
    "        is_travel_verb = 1\n",
    "    else:\n",
    "        is_travel_verb = 0\n",
    "    \n",
    "    entry =  word_tokenize(row['sentence'])\n",
    "    \n",
    "    if not isinstance(row['Travel Noun Phrases'],float):\n",
    "        if row['Location'] in row['Travel Noun Phrases']:\n",
    "            is_in_travel_phrase = 1\n",
    "        else:\n",
    "            is_in_travel_phrase = 0\n",
    "    else:\n",
    "        is_in_travel_phrase = 0\n",
    "    \n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    #print(str(Final_words))\n",
    "    data.loc[index,'isInSection'] = int(is_in_section)\n",
    "    data.loc[index,'isTravelVerb'] = int(is_travel_verb)\n",
    "    data.loc[index,'isInTravelPhrase'] = int(is_in_travel_phrase)\n",
    "    data.loc[index,'sentence_final'] = str(Final_words)  \n",
    "\n",
    "#column_trans = ColumnTransformer(\n",
    "#    [('section_title', OneHotEncoder(dtype='int'),['sectionTitle']),\n",
    "#      ('sentence', TfidfVectorizer(), 'sentence_final')],\n",
    "#     remainder='drop')\n",
    "\n",
    "\n",
    "features = data[['isInSection','isTravelVerb','isInTravelPhrase','sentence_final']].copy()\n",
    "#features = data[['isInSection','isTravelVerb','isInTravelPhrase','sentence_final']].copy()\n",
    "\n",
    "print(features.dtypes)\n",
    "target = data.Result.values\n",
    "#print(features)    \n",
    "print(\"Okay Done\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(features,target,test_size=0.3)\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB score: 0.433333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "numerical_features = features.dtypes == 'float'\n",
    "#model = naive_bayes.MultinomialNB()\n",
    "model = SVC(gamma='auto')\n",
    "#transformer = ColumnTransformer(transformers=[('isInSection', OneHotEncoder(), [0, 1])])\n",
    "\n",
    "transformer = ColumnTransformer(transformers=[('sentence_tfidf', TfidfVectorizer(), 'sentence_final')])\n",
    "\n",
    "\n",
    "pipe = Pipeline(steps=[('t', transformer), ('m',model)])\n",
    "\n",
    "#model.fit_transform(features)\n",
    "\n",
    "pipe.fit(Train_X, Train_Y)\n",
    "print(\"Multinomial NB score: %f\" % pipe.score(Test_X, Test_Y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
