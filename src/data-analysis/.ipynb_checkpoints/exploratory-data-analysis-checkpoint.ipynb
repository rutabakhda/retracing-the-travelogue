{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "basepath = os.path.dirname(os.path.abspath('__file__'))\n",
    "datapath = basepath + '/data/hugh-murray/'\n",
    "readfile1 = 'chapter1/chapter1-processed.csv'\n",
    "readfile2 = 'chapter2/chapter2-processed.csv'\n",
    "readfile3 = 'chapter3/chapter3-processed.csv'\n",
    "readfile_total = 'chapters-combined-processed.csv'\n",
    "\n",
    "df1 = pd.read_csv(datapath + readfile1,sep='\\t',encoding='ISO-8859-1')\n",
    "df2 = pd.read_csv(datapath + readfile2,sep='\\t',encoding='ISO-8859-1')\n",
    "df3 = pd.read_csv(datapath + readfile3,sep='\\t',encoding='ISO-8859-1')\n",
    "df_total = pd.read_csv(datapath + readfile_total,encoding='ISO-8859-1')\n",
    "df_total.head(10)\n",
    "\n",
    "def get_sections(df):\n",
    "    return df.groupby(\"sectionNo\").count()\n",
    "\n",
    "def get_records(df):\n",
    "    return df.shape[0]\n",
    "\n",
    "def average_sentence_per_section(df):\n",
    "    return df.groupby(\"sectionNo\").count().mean()\n",
    "\n",
    "chapter1_sections = get_sections(df1)\n",
    "chapter1_records = get_records(df1)\n",
    "chapter1_sentence_average = average_sentence_per_section(df1)\n",
    "\n",
    "list_sections = [get_sections(df1),get_sections(df2), get_sections(df3), get_sections(df_total)]\n",
    "list_records = [get_records(df1), get_records(df2), get_records(df3), get_records(df_total)]\n",
    "list_sentence_average = [average_sentence_per_section(df1), average_sentence_per_section(df2), average_sentence_per_section(df3), average_sentence_per_section(df_total)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_top_n_words(corpus, n=None,n_gram=None):\n",
    "    vec = CountVectorizer(ngram_range=(n_gram,n_gram)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    total_words =sum(freq for word,freq in word_freq)\n",
    "    total_unique_words = \n",
    "    return words_freq[:n]\n",
    "\n",
    "def show_plot(common_words):\n",
    "    df1 = pd.DataFrame(common_words, columns = ['cleanedText' , 'count'])\n",
    "    df1.groupby('cleanedText').sum()['count'].sort_values(ascending=False).plot.bar()\n",
    "    plt.show()\n",
    "    \n",
    "common_words_one_gram = get_top_n_words(df1['sentence_cleaned'], 20,1)\n",
    "show_plot(common_words_one_gram)\n",
    "\n",
    "common_words_bi_gram = get_top_n_words(df['sentence_cleaned'], 20,2)\n",
    "show_plot(common_words_bi_gram)\n",
    "\n",
    "common_words_tri_gram = get_top_n_words(df['sentence_cleaned'], 20,3)\n",
    "show_plot(common_words_tri_gram)\n",
    "\n",
    "#for word, freq in common_words:\n",
    "#    print(word, freq)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(str(df['sentence']))\n",
    "pos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])\n",
    "pos_df = pos_df.pos.value_counts()[:20]\n",
    "pos_df.plot.bar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_len'] = df['sentence'].astype(str).apply(len)\n",
    "df['word_count'] = df['sentence'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "df['review_len'].plot(\n",
    "    kind='hist',\n",
    "    bins=100,\n",
    "    x='review length',\n",
    "    y='count',\n",
    "    title='Review Text Length Distribution')\n",
    "\n",
    "df['word_count'].plot(\n",
    "    kind='hist',\n",
    "    bins=100,\n",
    "    x='review length',\n",
    "    y='count',\n",
    "    title='Review Text Length Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
